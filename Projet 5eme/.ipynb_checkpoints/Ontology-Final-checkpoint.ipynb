{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T11:48:23.152084Z",
     "start_time": "2021-10-31T11:48:22.348980Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import pdfplumber\n",
    "import joblib\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd #for handling csv and csv contents\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace #basic RDF handling\n",
    "from rdflib.namespace import FOAF , XSD #most common namespaces\n",
    "import urllib.parse #for parsing strings to URI's\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets get the data and preprocess it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:40:33.946878Z",
     "start_time": "2021-10-31T10:40:33.933911Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(deb, fin):\n",
    "    pdf = pdfplumber.open('docs/PMBOK 5th.pdf')\n",
    "    text=''\n",
    "    for i in range(deb,fin):\n",
    "        page = pdf.pages[i]\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:42:00.694154Z",
     "start_time": "2021-10-31T10:40:33.948871Z"
    }
   },
   "outputs": [],
   "source": [
    "P1 = load_data(334, 344)\n",
    "\n",
    "P2 = load_data(344, 353)\n",
    "\n",
    "P3 = load_data(353, 359)\n",
    "\n",
    "P4 = load_data(359, 368)\n",
    "\n",
    "P5 = load_data(367, 374)\n",
    "\n",
    "P6 = load_data(374, 381)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:42:00.710112Z",
     "start_time": "2021-10-31T10:42:00.697145Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Remove numbers\n",
    "    text = re.sub('[\\d]+','',text)\n",
    "    text = re.sub('[\\t\\n\\r\\f\\v]',' ',text)\n",
    "    text = re.sub('[•-]','',text)\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "    text = text.replace('&',\"and\").replace('©',\"\").replace('—The',\" is the\").lower()\n",
    "    text = text.replace('  project management institute. a guide to the project management body of knowledge (pmbok® guide) – fifth edition licensed to: jorge diego fuentes sanchez pmi memberid: this copy is a pmi member benefit, not for distribution, sale, or reproduction.','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:42:00.755987Z",
     "start_time": "2021-10-31T10:42:00.713103Z"
    }
   },
   "outputs": [],
   "source": [
    "P1_clean = clean(P1)\n",
    "\n",
    "P2_clean = clean(P2)\n",
    "\n",
    "P3_clean = clean(P3)\n",
    "\n",
    "P4_clean = clean(P4)\n",
    "\n",
    "P5_clean = clean(P5)\n",
    "\n",
    "P6_clean = clean(P6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T11:42:03.225126Z",
     "start_time": "2021-10-31T11:42:03.218145Z"
    }
   },
   "source": [
    "# Now we have to split our data into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:46:46.951181Z",
     "start_time": "2021-10-31T10:46:46.911290Z"
    }
   },
   "outputs": [],
   "source": [
    "P1_SENT = nltk.tokenize.sent_tokenize(P1_clean)\n",
    "P2_SENT = nltk.tokenize.sent_tokenize(P2_clean)\n",
    "P3_SENT = nltk.tokenize.sent_tokenize(P3_clean)\n",
    "P4_SENT = nltk.tokenize.sent_tokenize(P4_clean)\n",
    "P5_SENT = nltk.tokenize.sent_tokenize(P5_clean)\n",
    "P6_SENT = nltk.tokenize.sent_tokenize(P6_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:49:19.235882Z",
     "start_time": "2021-10-31T10:49:19.208954Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'sentence': P1_SENT})\n",
    "df2 = pd.DataFrame({'sentence': P2_SENT})\n",
    "df3 = pd.DataFrame({'sentence': P3_SENT})\n",
    "df4 = pd.DataFrame({'sentence': P4_SENT})\n",
    "df5 = pd.DataFrame({'sentence': P5_SENT})\n",
    "df6 = pd.DataFrame({'sentence': P6_SENT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity and Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:49:26.442920Z",
     "start_time": "2021-10-31T10:49:26.423942Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"  # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"  # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    #############################################################\n",
    "\n",
    "    for tok in nlp(sent):\n",
    "        ## chunk 2\n",
    "        # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "            # check: token is a compound word or not\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    prefix = prv_tok_text + \" \" + tok.text\n",
    "\n",
    "            # check: token is a modifier or not\n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    modifier = prv_tok_text + \" \" + tok.text\n",
    "\n",
    "            ## chunk 3\n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = modifier + \" \" + prefix + \" \" + tok.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            ## chunk 4\n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = modifier + \" \" + prefix + \" \" + tok.text\n",
    "\n",
    "            ## chunk 5\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "    #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to extract these entity pairs for all the sentences in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:54:01.247166Z",
     "start_time": "2021-10-31T10:54:01.234202Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_entity(df):\n",
    "    entity_pairs = []\n",
    "    for i in df[\"sentence\"]:\n",
    "        entity_pairs.append(get_entities(i))\n",
    "    return entity_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:54:09.053868Z",
     "start_time": "2021-10-31T10:54:02.390195Z"
    }
   },
   "outputs": [],
   "source": [
    "entity_pairs1 = make_entity(df1)\n",
    "entity_pairs2 = make_entity(df2)\n",
    "entity_pairs3 = make_entity(df3)\n",
    "entity_pairs4 = make_entity(df4)\n",
    "entity_pairs5 = make_entity(df5)\n",
    "entity_pairs6 = make_entity(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation / Predicate Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:54:28.408783Z",
     "start_time": "2021-10-31T10:54:28.400805Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    #define the pattern\n",
    "    pattern = [ {'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"} ] \n",
    "\n",
    "    matcher.add(\"matching_1\", None, pattern)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]]\n",
    "\n",
    "    return (span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T10:56:19.732907Z",
     "start_time": "2021-10-31T10:56:13.196615Z"
    }
   },
   "outputs": [],
   "source": [
    "relations1 = [get_relation(i) for i in df1['sentence']]\n",
    "relations2 = [get_relation(i) for i in df2['sentence']]\n",
    "relations3 = [get_relation(i) for i in df3['sentence']]\n",
    "relations4 = [get_relation(i) for i in df4['sentence']]\n",
    "relations5 = [get_relation(i) for i in df5['sentence']]\n",
    "relations6 = [get_relation(i) for i in df6['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our Datasets [*'subject'* , *'relation'* , *'object'*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T11:01:36.483789Z",
     "start_time": "2021-10-31T11:01:36.472816Z"
    }
   },
   "outputs": [],
   "source": [
    "subject1 = [i[0] for i in entity_pairs1]\n",
    "subject2 = [i[0] for i in entity_pairs2]\n",
    "subject3 = [i[0] for i in entity_pairs3]\n",
    "subject4 = [i[0] for i in entity_pairs4]\n",
    "subject5 = [i[0] for i in entity_pairs5]\n",
    "subject6 = [i[0] for i in entity_pairs6]\n",
    "\n",
    "target1 = [i[1] for i in entity_pairs1]\n",
    "target2 = [i[1] for i in entity_pairs2]\n",
    "target3 = [i[1] for i in entity_pairs3]\n",
    "target4 = [i[1] for i in entity_pairs4]\n",
    "target5 = [i[1] for i in entity_pairs5]\n",
    "target6 = [i[1] for i in entity_pairs6]\n",
    "\n",
    "df1 = pd.DataFrame({'subject':subject1, 'property':relations1, 'object':target1 })\n",
    "df2 = pd.DataFrame({'subject':subject2, 'property':relations2, 'object':target2 })\n",
    "df3 = pd.DataFrame({'subject':subject3, 'property':relations3, 'object':target3 })\n",
    "df4 = pd.DataFrame({'subject':subject4, 'property':relations4, 'object':target4 })\n",
    "df5 = pd.DataFrame({'subject':subject5, 'property':relations5, 'object':target5 })\n",
    "df6 = pd.DataFrame({'subject':subject6, 'property':relations6, 'object':target6 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()\n",
    "df3 = df3.dropna()\n",
    "df4 = df4.dropna()\n",
    "df5 = df5.dropna()\n",
    "df6 = df6.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Namesapeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T11:48:30.218046Z",
     "start_time": "2021-10-31T11:48:30.201092Z"
    }
   },
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "ppl = Namespace('http://example.org/people/')\n",
    "loc = Namespace('http://mylocations.org/addresses/')\n",
    "schema = Namespace('http://schema.org/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['subject'] = df['subject'].apply(lambda x: x.replace(' ','_'))\n",
    "    df['property'] = df['property'].apply(lambda x: x.replace(' ','_'))\n",
    "    df['object'] = df['object'].apply(lambda x: x.replace(' ','_'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = process(df1)\n",
    "df2 = process(df2)\n",
    "df3 = process(df3)\n",
    "df4 = process(df4)\n",
    "df5 = process(df5)\n",
    "df6 = process(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ontology(df,name):\n",
    "    for index, row in df.iterrows():\n",
    "        if name == 'Person':\n",
    "            g.add((URIRef(ppl+row['subject']), RDF.type, FOAF.Person))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'subject'), Literal(row['subject'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'property'), Literal(row['property'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'object'), Literal(row['object'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(loc+urllib.parse.quote(row['object'])), URIRef(schema+'subject'), Literal(row['property'], datatype=XSD.string) ))\n",
    "        if name == 'Document':\n",
    "            g.add((URIRef(ppl+row['subject']), RDF.type, FOAF.Document))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'subject'), Literal(row['subject'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'property'), Literal(row['property'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'object'), Literal(row['object'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(loc+urllib.parse.quote(row['object'])), URIRef(schema+'subject'), Literal(row['property'], datatype=XSD.string) ))\n",
    "        elif name == 'Agent':\n",
    "            g.add((URIRef(ppl+row['subject']), RDF.type, FOAF.Agent))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'subject'), Literal(row['subject'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'property'), Literal(row['property'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'object'), Literal(row['object'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(loc+urllib.parse.quote(row['object'])), URIRef(schema+'subject'), Literal(row['property'], datatype=XSD.string) ))\n",
    "        elif name == 'Image':\n",
    "            g.add((URIRef(ppl+row['subject']), RDF.type, FOAF.Image))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'subject'), Literal(row['subject'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'property'), Literal(row['property'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'object'), Literal(row['object'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(loc+urllib.parse.quote(row['object'])), URIRef(schema+'subject'), Literal(row['property'], datatype=XSD.string) ))\n",
    "        elif name == 'Organization':\n",
    "            g.add((URIRef(ppl+row['subject']), RDF.type, FOAF.Organization))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'subject'), Literal(row['subject'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'property'), Literal(row['property'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'object'), Literal(row['object'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(loc+urllib.parse.quote(row['object'])), URIRef(schema+'subject'), Literal(row['property'], datatype=XSD.string) ))\n",
    "        elif name == 'Project':\n",
    "            g.add((URIRef(ppl+row['subject']), RDF.type, FOAF.Project))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'subject'), Literal(row['subject'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'property'), Literal(row['property'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(ppl+row['subject']), URIRef(schema+'object'), Literal(row['object'], datatype=XSD.string) ))\n",
    "            g.add((URIRef(loc+urllib.parse.quote(row['object'])), URIRef(schema+'subject'), Literal(row['property'], datatype=XSD.string) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ontology(df1,'Person')\n",
    "create_ontology(df2,'Document')\n",
    "create_ontology(df3,'Agent')\n",
    "create_ontology(df4,'Image')\n",
    "create_ontology(df5,'Organization')\n",
    "create_ontology(df6,'Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.serialize('ontoD.owl',format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T11:50:30.782305Z",
     "start_time": "2021-10-31T11:50:30.723464Z"
    }
   },
   "source": [
    "![caption](files/giphy.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

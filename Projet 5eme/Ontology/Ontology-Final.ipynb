{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-07T18:35:28.549790Z",
     "start_time": "2021-11-07T18:35:21.661659Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import bs4\n",
    "import requests\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import pdfplumber\n",
    "import joblib\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd #for handling csv and csv contents\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace #basic RDF handling\n",
    "from rdflib.namespace import FOAF , XSD #most common namespaces\n",
    "import urllib.parse #for parsing strings to URI's\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets get the data and preprocess it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:18:57.502205Z",
     "start_time": "2021-11-02T09:18:57.475277Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(deb, fin):\n",
    "    pdf = pdfplumber.open('docs/PMBOK 5th.pdf')\n",
    "    text=''\n",
    "    for i in range(deb,fin):\n",
    "        page = pdf.pages[i]\n",
    "        text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:21.991639Z",
     "start_time": "2021-11-02T09:18:57.506194Z"
    }
   },
   "outputs": [],
   "source": [
    "P1 = load_data(334, 344)\n",
    "\n",
    "P2 = load_data(344, 353)\n",
    "\n",
    "P3 = load_data(353, 359)\n",
    "\n",
    "P4 = load_data(359, 368)\n",
    "\n",
    "P5 = load_data(367, 374)\n",
    "\n",
    "P6 = load_data(374, 381)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:22.007598Z",
     "start_time": "2021-11-02T09:20:21.995628Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    # Remove numbers\n",
    "    text = re.sub('[\\d]+','',text)\n",
    "    text = re.sub('[\\t\\n\\r\\f\\v]',' ',text)\n",
    "    text = re.sub('[•-]','',text)\n",
    "    text = re.sub(\"\\s\\s+\" , \" \", text)\n",
    "    text = text.replace('&',\"and\").replace('©',\"\").replace('—The',\" is the\").lower()\n",
    "    text = text.replace('  project management institute. a guide to the project management body of knowledge (pmbok® guide) – fifth edition licensed to: jorge diego fuentes sanchez pmi memberid: this copy is a pmi member benefit, not for distribution, sale, or reproduction.','')\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:22.037516Z",
     "start_time": "2021-11-02T09:20:22.011593Z"
    }
   },
   "outputs": [],
   "source": [
    "P1_clean = clean(P1)\n",
    "\n",
    "P2_clean = clean(P2)\n",
    "\n",
    "P3_clean = clean(P3)\n",
    "\n",
    "P4_clean = clean(P4)\n",
    "\n",
    "P5_clean = clean(P5)\n",
    "\n",
    "P6_clean = clean(P6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T11:42:03.225126Z",
     "start_time": "2021-10-31T11:42:03.218145Z"
    }
   },
   "source": [
    "# Now we have to split our data into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:22.162183Z",
     "start_time": "2021-11-02T09:20:22.040509Z"
    }
   },
   "outputs": [],
   "source": [
    "P1_SENT = nltk.tokenize.sent_tokenize(P1_clean)\n",
    "P2_SENT = nltk.tokenize.sent_tokenize(P2_clean)\n",
    "P3_SENT = nltk.tokenize.sent_tokenize(P3_clean)\n",
    "P4_SENT = nltk.tokenize.sent_tokenize(P4_clean)\n",
    "P5_SENT = nltk.tokenize.sent_tokenize(P5_clean)\n",
    "P6_SENT = nltk.tokenize.sent_tokenize(P6_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:22.193100Z",
     "start_time": "2021-11-02T09:20:22.165175Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'sentence': P1_SENT})\n",
    "df2 = pd.DataFrame({'sentence': P2_SENT})\n",
    "df3 = pd.DataFrame({'sentence': P3_SENT})\n",
    "df4 = pd.DataFrame({'sentence': P4_SENT})\n",
    "df5 = pd.DataFrame({'sentence': P5_SENT})\n",
    "df6 = pd.DataFrame({'sentence': P6_SENT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity and Relation Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:22.224018Z",
     "start_time": "2021-11-02T09:20:22.197090Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"  # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"  # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    #############################################################\n",
    "\n",
    "    for tok in nlp(sent):\n",
    "        ## chunk 2\n",
    "        # if token is a punctuation mark then move on to the next token\n",
    "        if tok.dep_ != \"punct\":\n",
    "            # check: token is a compound word or not\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    prefix = prv_tok_text + \" \" + tok.text\n",
    "\n",
    "            # check: token is a modifier or not\n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "                # if the previous word was also a 'compound' then add the current word to it\n",
    "                if prv_tok_dep == \"compound\":\n",
    "                    modifier = prv_tok_text + \" \" + tok.text\n",
    "\n",
    "            ## chunk 3\n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = modifier + \" \" + prefix + \" \" + tok.text\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prv_tok_dep = \"\"\n",
    "                prv_tok_text = \"\"\n",
    "\n",
    "            ## chunk 4\n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = modifier + \" \" + prefix + \" \" + tok.text\n",
    "\n",
    "            ## chunk 5\n",
    "            # update variables\n",
    "            prv_tok_dep = tok.dep_\n",
    "            prv_tok_text = tok.text\n",
    "    #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this function to extract these entity pairs for all the sentences in our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:22.238979Z",
     "start_time": "2021-11-02T09:20:22.227010Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_entity(df):\n",
    "    entity_pairs = []\n",
    "    for i in df[\"sentence\"]:\n",
    "        entity_pairs.append(get_entities(i))\n",
    "    return entity_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:32.182194Z",
     "start_time": "2021-11-02T09:20:22.245958Z"
    }
   },
   "outputs": [],
   "source": [
    "entity_pairs1 = make_entity(df1)\n",
    "entity_pairs2 = make_entity(df2)\n",
    "entity_pairs3 = make_entity(df3)\n",
    "entity_pairs4 = make_entity(df4)\n",
    "entity_pairs5 = make_entity(df5)\n",
    "entity_pairs6 = make_entity(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation / Predicate Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:32.198155Z",
     "start_time": "2021-11-02T09:20:32.184195Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    #define the pattern\n",
    "    pattern = [ {'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"} ] \n",
    "\n",
    "    matcher.add(\"matching_1\", None, pattern)\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]]\n",
    "\n",
    "    return (span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:43.521545Z",
     "start_time": "2021-11-02T09:20:32.201148Z"
    }
   },
   "outputs": [],
   "source": [
    "relations1 = [get_relation(i) for i in df1['sentence']]\n",
    "relations2 = [get_relation(i) for i in df2['sentence']]\n",
    "relations3 = [get_relation(i) for i in df3['sentence']]\n",
    "relations4 = [get_relation(i) for i in df4['sentence']]\n",
    "relations5 = [get_relation(i) for i in df5['sentence']]\n",
    "relations6 = [get_relation(i) for i in df6['sentence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building our Datasets [*'subject'* , *'relation'* , *'object'*]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:43.553345Z",
     "start_time": "2021-11-02T09:20:43.523547Z"
    }
   },
   "outputs": [],
   "source": [
    "subject1 = [i[0] for i in entity_pairs1]\n",
    "subject2 = [i[0] for i in entity_pairs2]\n",
    "subject3 = [i[0] for i in entity_pairs3]\n",
    "subject4 = [i[0] for i in entity_pairs4]\n",
    "subject5 = [i[0] for i in entity_pairs5]\n",
    "subject6 = [i[0] for i in entity_pairs6]\n",
    "\n",
    "target1 = [i[1] for i in entity_pairs1]\n",
    "target2 = [i[1] for i in entity_pairs2]\n",
    "target3 = [i[1] for i in entity_pairs3]\n",
    "target4 = [i[1] for i in entity_pairs4]\n",
    "target5 = [i[1] for i in entity_pairs5]\n",
    "target6 = [i[1] for i in entity_pairs6]\n",
    "\n",
    "df1 = pd.DataFrame({'subject':subject1, 'property':relations1, 'object':target1 })\n",
    "df2 = pd.DataFrame({'subject':subject2, 'property':relations2, 'object':target2 })\n",
    "df3 = pd.DataFrame({'subject':subject3, 'property':relations3, 'object':target3 })\n",
    "df4 = pd.DataFrame({'subject':subject4, 'property':relations4, 'object':target4 })\n",
    "df5 = pd.DataFrame({'subject':subject5, 'property':relations5, 'object':target5 })\n",
    "df6 = pd.DataFrame({'subject':subject6, 'property':relations6, 'object':target6 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:43.583592Z",
     "start_time": "2021-11-02T09:20:43.555344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>property</th>\n",
       "      <th>object</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>project risk management</td>\n",
       "      <td>includes</td>\n",
       "      <td>response project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>objectives</td>\n",
       "      <td>are</td>\n",
       "      <td>negative project risk project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>risk management which</td>\n",
       "      <td>provides</td>\n",
       "      <td>risk management processes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plan risk management</td>\n",
       "      <td>is</td>\n",
       "      <td>how risk management project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td></td>\n",
       "      <td>reporting</td>\n",
       "      <td>formats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>how  outcomes</td>\n",
       "      <td>define</td>\n",
       "      <td>risk management process</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>it</td>\n",
       "      <td>describes</td>\n",
       "      <td>risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td></td>\n",
       "      <td>tracking</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>how risk management processes</td>\n",
       "      <td>tracking</td>\n",
       "      <td>current  project</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           subject   property                         object\n",
       "0          project risk management   includes               response project\n",
       "1                       objectives        are  negative project risk project\n",
       "2            risk management which   provides      risk management processes\n",
       "3             plan risk management         is    how risk management project\n",
       "4                                           .                               \n",
       "..                             ...        ...                            ...\n",
       "248                                 reporting                        formats\n",
       "249                  how  outcomes     define        risk management process\n",
       "250                             it  describes                           risk\n",
       "251                                  tracking                               \n",
       "252  how risk management processes   tracking               current  project\n",
       "\n",
       "[253 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Nan Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:43.599550Z",
     "start_time": "2021-11-02T09:20:43.585585Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()\n",
    "df3 = df3.dropna()\n",
    "df4 = df4.dropna()\n",
    "df5 = df5.dropna()\n",
    "df6 = df6.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Namesapeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:43.615505Z",
     "start_time": "2021-11-02T09:20:43.602541Z"
    }
   },
   "outputs": [],
   "source": [
    "g = Graph()\n",
    "ppl = Namespace('http://example.org/people/')\n",
    "loc = Namespace('http://mylocations.org/addresses/')\n",
    "schema = Namespace('http://schema.org/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:43.630466Z",
     "start_time": "2021-11-02T09:20:43.618499Z"
    }
   },
   "outputs": [],
   "source": [
    "def process(df):\n",
    "    df['subject'] = df['subject'].apply(lambda x: x.replace(' ','_'))\n",
    "    df['property'] = df['property'].apply(lambda x: x.replace(' ','_'))\n",
    "    df['object'] = df['object'].apply(lambda x: x.replace(' ','_'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:20:43.661304Z",
     "start_time": "2021-11-02T09:20:43.632461Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = process(df1)\n",
    "df2 = process(df2)\n",
    "df3 = process(df3)\n",
    "df4 = process(df4)\n",
    "df5 = process(df5)\n",
    "df6 = process(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:45:49.071442Z",
     "start_time": "2021-11-02T09:45:49.049502Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_ontology(df):\n",
    "    for index, row in df.iterrows():\n",
    "        g.add((URIRef(ppl+row['subject']), RDF.type, FOAF.Document))\n",
    "        g.add((URIRef(ppl+row['subject']), URIRef(schema+'subject'), Literal(row['subject'], datatype=XSD.string) ))\n",
    "        g.add((URIRef(ppl+row['subject']), URIRef(schema+'property'), Literal(row['property'], datatype=XSD.string) ))\n",
    "        g.add((URIRef(ppl+row['subject']), URIRef(schema+'object'), Literal(row['object'], datatype=XSD.string) ))\n",
    "        g.add((URIRef(loc+urllib.parse.quote(row['object'])), URIRef(schema+'subject'), Literal(row['property']\n",
    "                                                                                                , datatype=XSD.string) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:45:50.643438Z",
     "start_time": "2021-11-02T09:45:50.299406Z"
    }
   },
   "outputs": [],
   "source": [
    "create_ontology(df1)\n",
    "create_ontology(df2)\n",
    "create_ontology(df3)\n",
    "create_ontology(df4)\n",
    "create_ontology(df5)\n",
    "create_ontology(df6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-02T09:45:57.882211Z",
     "start_time": "2021-11-02T09:45:57.523141Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Graph identifier=N202f23d3260443c89009b1ce4e1242fa (<class 'rdflib.graph.Graph'>)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.serialize('pmbok.owl',format='turtle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T11:50:30.782305Z",
     "start_time": "2021-10-31T11:50:30.723464Z"
    }
   },
   "source": [
    "![caption](files/giphy.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
